{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project #2 : Continuous Control\n",
    "\n",
    "## DDPG Training Algorithm\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements my solution for the Continous Control project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "I implemented the Deep Deterministic Policy Gradient (DDPG) training algorithm for the Unity Reacher environment.\n",
    "\n",
    "The code in this notebook (Continuous_Control.ipynb), the ddpg_agent.py, and model.py files is derived from the 'ddpg-pendulum' code example used in the nanodegree program. The code was modified to successfully train both the single agent and multiple agents version of the the Unity Reacher environment. The code also implements the logic to stop the training when the environment is considered solved. The project requirement is to reach an average score of 30, measured over the last 100 training episodes. In the case of the multiple agents version, the episode score is the mean of the agents' scores of the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Python Packages\n",
    "\n",
    "If you haven't done so already, run this command to install a few packages in your computing environment.  This line may take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "#import random\n",
    "import time\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate the Unity Reacher environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# on my iMac \n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "\n",
    "# in my Udacity Workspace\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "\n",
    "# on my AWS instance\n",
    "#env = UnityEnvironment(file_name='./Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents    : 20\n",
      "Size of each action :  4\n",
      "Size of each state  : 33\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment and activate the training mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents    :', num_agents)\n",
    "\n",
    "# action size\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action : ', action_size)\n",
    "\n",
    "# state size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('Size of each state  :', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Instantiate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   *****************\n",
      "   *** Using CPU ***\n",
      "   *****************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from ddpg_agent import DDPG_Agent\n",
    "#agent = DDPG_Agent(state_size, action_size, num_agents)\n",
    "from ddpg_agent import DDPG_Agent\n",
    "agent = DDPG_Agent(state_size, action_size, num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tScore: 0.7\t Average Score: 0.2\tClock: 121\n",
      "Episode 20\tScore: 0.8\t Average Score: 0.4\tClock: 364\n",
      "Episode 26\tScore: 0.8\tAverage Score: 0.5\tClock: 537"
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "n_episodes = 5000   # maximum number of episodes\n",
    "max_t = 1000        # maximum number of timesteps per episode\n",
    "target_score = 30   # target score; training is complete when target score is met\n",
    "window_size = 100   # number of episodes over which the average episode score is calculated\n",
    "print_every = 10    # number of episodes after which the training procedures prints out a persistent update\n",
    "    \n",
    "# variables to track progress of the training process\n",
    "scores_deque = deque(maxlen = window_size) # deque for the last window_size episode scores\n",
    "scores = []                                # container for all episode scores\n",
    "best_score = -np.Inf                       # the highest episode score achieved\n",
    "start_time =  time.time()                  # start time, required to calculate the total training time\n",
    "\n",
    "# training loop\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "    # reset the environment and obtain the initial state\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "\n",
    "    # reset the agent\n",
    "    agent.reset()\n",
    "\n",
    "    # reset the score for each Reacher agent\n",
    "    agent_scores = np.zeros(num_agents)\n",
    "\n",
    "    # run an experience, one on each Reacher agent\n",
    "    for t in range(max_t):\n",
    "        \n",
    "        # send state to each agent and collect their action\n",
    "        actions = agent.act(states)\n",
    "\n",
    "        # send actions to the environments\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "\n",
    "        # get next states\n",
    "        next_states = env_info.vector_observations\n",
    "\n",
    "        # get rewards\n",
    "        rewards = env_info.rewards\n",
    "\n",
    "        # get dones (True if agent reached the terminal state)\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # Save experiences in the replay memory,\n",
    "        # and sample a batch of experiences from the replay memory to train the agent\n",
    "        agent.step(states, actions, rewards, next_states, dones)\n",
    "        #agent.step(states=states, actions=actions, rewards=rewards, next_states=next_states, dones=dones)\n",
    "\n",
    "        # update the episode score and roll the states over for the next time step\n",
    "        agent_scores += np.array(rewards)\n",
    "        states = next_states\n",
    "\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "    # calculate the average episode score across all agent\n",
    "    episode_score = np.mean(agent_scores)\n",
    "\n",
    "    # update the sliding window of episode scores\n",
    "    scores_deque.append(episode_score)\n",
    "\n",
    "    # append the average episode score to the scores array \n",
    "    scores.append(episode_score)\n",
    "\n",
    "    # update the average sliding score\n",
    "    average_score = np.mean(scores_deque)\n",
    "\n",
    "    # update time the training process has lasted thus far, in minutes\n",
    "    total_training_time = time.time() - start_time\n",
    "\n",
    "    # after each episode, update the printout of the episode and sliding average scores\n",
    "    print('\\rEpisode {}\\tScore: {:.1f}\\tAverage Score: {:.1f}\\tClock: {}'\n",
    "        .format(i_episode, episode_score, np.mean(scores_deque), int(total_training_time)), end=\"\")\n",
    "\n",
    "    # after every print_every episode, print the sliding average score\n",
    "    if i_episode % print_every == 0:\n",
    "        print('\\rEpisode {}\\tScore: {:.1f}\\t Average Score: {:.1f}\\tClock: {}'\n",
    "        .format(i_episode, episode_score, np.mean(scores_deque), int(total_training_time)))\n",
    "\n",
    "    # save the model weights if the average episode score improved\n",
    "    if (i_episode >= window_size) and (episode_score > best_score):\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        best_score = episode_score\n",
    "\n",
    "    if average_score >= target_score:\n",
    "        print('\\n\\nThe agent trained in {:d} episodes! \\n\\n\\tHighest score\\t{:.0f}\\n\\tAverage score\\t{:.1f} (measured over last {:d} episodes)\\n'\n",
    "              .format(i_episode - window_size, best_score, average_score, window_size))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot and save the Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig.suptitle(\"Learning Curve\", fontsize=15)\n",
    "ax.plot(np.arange(1, len(scores)+1), scores, color=\"forestgreen\", label=\"line\")\n",
    "plt.xlim(left=1)\n",
    "plt.ylim(bottom=0)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.grid(True)\n",
    "ax.grid(b=True, which='major', color='gray', linestyle='-', linewidth=0.75)\n",
    "ax.minorticks_on()\n",
    "ax.grid(b=True, which='minor', color='gray', linestyle='-', alpha=.75, linewidth=0.25)\n",
    "plt.show()\n",
    "\n",
    "#  save plot to a file\n",
    "fig.savefig(\"ddpg_agent_learning_curve.jpg\")"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
