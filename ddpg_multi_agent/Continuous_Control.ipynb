{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project #2 : Continuous Control\n",
    "\n",
    "## DDPG Multi Agent Implementation\n",
    "\n",
    "---\n",
    "\n",
    "This notebook implements my solution for the Continous Control project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Python Packages\n",
    "\n",
    "If needed, run this command to install a few packages in your computing environment.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "#import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from unityagents import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Instantiate the Unity Reacher environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# on my iMac \n",
    "#env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "\n",
    "# in my Udacity Workspace\n",
    "#env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "\n",
    "# on my AWS instance\n",
    "env = UnityEnvironment(file_name='./Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n"
     ]
    }
   ],
   "source": [
    "# reset the environment and activate the training mode\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Instantiate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "   *** Using GPU ***\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ddpg_agent import DDPG_MultiAgent\n",
    "agent = DDPG_MultiAgent(state_size, action_size, num_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=20, max_t=200):\n",
    "    \n",
    "    ''' Deep Deterministic Policy Gradient (DDPG) Algorithm for multiple agents.\n",
    "        The code is derived from the 'ddpg-pendulum' code provided by Udacity.\n",
    "        The code is modified to work with the Unity Reacher environment for multiple agents,\n",
    "        calculate the average score across agents and a sliding window of epsiodes, and \n",
    "        stop the algorithm when the target average score is reached.\n",
    "        \n",
    "        Arguments:\n",
    "            n_episodes : number of episodes\n",
    "            max_t      : maximum number of timesteps per episode\n",
    "        '''\n",
    "    \n",
    "    # training parameters\n",
    "    k_target_score = 30.0  # Score that must be met to be considered solved\n",
    "    k_sliding_window = 100 # number of episodes used to calculate mean to check if solution satisified\n",
    "    k_print_every = 1     # How often to keep a persistent version of printout\n",
    "\n",
    "    # variables to track progress\n",
    "    scores_deque = deque(maxlen = k_sliding_window) # store scores of last k_sliding_window episodes\n",
    "    scores = []           # store score of every episode\n",
    "    max_average_score = -np.Inf   # the highest average episode score achieved across all episodes\n",
    "    \n",
    "    # start time to track total training time\n",
    "    t0 =  time.time()\n",
    "    \n",
    "    # training loop\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        \n",
    "        # reset the environments\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        # reset the agent\n",
    "        agent.reset()\n",
    "        \n",
    "        # reset the episode score for each agent\n",
    "        episode_scores = 0\n",
    "        \n",
    "        # run an experience on each agent\n",
    "        for t in range(max_t):\n",
    "            # send state to each agent and collect their action\n",
    "            actions = agent.act(states)\n",
    "            \n",
    "            # send actions to the environments\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            # get next states\n",
    "            next_states = env_info.vector_observations\n",
    "            \n",
    "            # get rewards\n",
    "            rewards = env_info.rewards\n",
    "            \n",
    "            # get dones (True if agent reached the terminal state)\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            # Save experiences in the replay memory,\n",
    "            # and sample a batch of experiences from the replay memory to train the agent\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            # update the episode score and roll the states over for the next time step\n",
    "            episode_scores += np.array(rewards)\n",
    "            states = next_states\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        # calculate the average episode score across all agent\n",
    "        average_episode_score = np.mean(episode_scores)\n",
    "        \n",
    "        # update the sliding window of episode scores\n",
    "        scores_deque.append(average_episode_score)\n",
    "        \n",
    "        # append the average episode score to the scores array \n",
    "        scores.append(average_episode_score)\n",
    "        \n",
    "        # update the average sliding score\n",
    "        average_sliding_score = np.mean(scores_deque)\n",
    "        \n",
    "        # update time the training process has lasted thus far, in minutes\n",
    "        total_training_time = (time.time()-t0)/60\n",
    "        \n",
    "        # after each episode, update the printout of the episode and sliding average scores\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.1f}\\tScore: {:.1f}\\tClock: '\n",
    "            .format(i_episode, np.mean(scores_deque), average_episode_score), int(total_training_time), end=\"\")\n",
    "        \n",
    "        # after every k_print_every episode, print the sliding average score\n",
    "        if i_episode % k_print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.1f}'\n",
    "                  .format(i_episode, average_sliding_score))\n",
    "        \n",
    "        # save the model weights if the average episode score improved\n",
    "        if (i_episode >= k_sliding_window) and (average_episode_score > max_average_score):\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            max_average_score = average_episode_score\n",
    "        \n",
    "        if average_sliding_score >= k_target_score:\n",
    "            print('\\n\\nAgents are trained in {:d} episodes!\\tAverage Score: {:.1f}\\n'\n",
    "                  .format(i_episode - k_sliding_window, average_score))\n",
    "            break\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.3\tScore: 0.3\tClock:  0\n",
      "Episode 2\tAverage Score: 0.5\tScore: 0.7\tClock:  0\n",
      "Episode 3\tAverage Score: 0.6\tScore: 0.8\tClock:  0\n",
      "Episode 4\tAverage Score: 0.6\tScore: 0.6\tClock:  0\n",
      "Episode 5\tAverage Score: 0.6\tScore: 0.7\tClock:  1\n",
      "Episode 6\tAverage Score: 0.6\tScore: 0.6\tClock:  1\n",
      "Episode 7\tAverage Score: 0.6\tScore: 0.5\tClock:  1\n",
      "Episode 8\tAverage Score: 0.5\tScore: 0.0\tClock:  1\n",
      "Episode 9\tAverage Score: 0.5\tScore: 0.0\tClock:  2\n",
      "Episode 10\tAverage Score: 0.4\tScore: 0.0\tClock:  2\n",
      "Episode 11\tAverage Score: 0.4\tScore: 0.0\tClock:  2\n",
      "Episode 12\tAverage Score: 0.4\tScore: 0.0\tClock:  3\n",
      "Episode 13\tAverage Score: 0.3\tScore: 0.0\tClock:  3\n",
      "Episode 14\tAverage Score: 0.3\tScore: 0.0\tClock:  3\n",
      "Episode 15\tAverage Score: 0.3\tScore: 0.0\tClock:  4\n",
      "Episode 16\tAverage Score: 0.3\tScore: 0.0\tClock:  4\n",
      "Episode 17\tAverage Score: 0.3\tScore: 0.0\tClock:  4\n"
     ]
    }
   ],
   "source": [
    "##### Train the agent #####\n",
    "\n",
    "n_episodes = 500    # maximum number of episodes\n",
    "max_t = 1000        # maximum number of timesteps per episode\n",
    "\n",
    "scores = train(n_episodes, max_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot the Training Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and save the learning curve\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "fig.suptitle(\"Learning Curve\", fontsize=15)\n",
    "ax.plot(np.arange(1, len(scores)+1), scores, color=\"#307EC7\", label=\"line\")\n",
    "ax.set_xlabel(\"Episode #\")\n",
    "ax.set_ylabel(\"Episode Score (average across agents)\")\n",
    "plt.xlim(left=1)\n",
    "plt.ylim(bottom=0)\n",
    "ax.grid(True)\n",
    "ax.grid(b=True, which='major', color='#999999', linestyle='-', linewidth=1)\n",
    "ax.minorticks_on()\n",
    "ax.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.7, linewidth=0.5)\n",
    "plt.show()\n",
    "fig.savefig(\"ddpg_agent_learning_curve.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
